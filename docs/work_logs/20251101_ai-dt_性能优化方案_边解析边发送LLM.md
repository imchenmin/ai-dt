# ai-dt 性能优化方案 - 边解析边发送LLM架构设计

**时间**: 2025-11-01
**问题**: ai-dt存在函数解析时间过长的问题，需要边解析边发送到LLM运行用例
**方案**: 流水线式生产者-消费者架构优化

## 🔍 性能瓶颈分析

### 当前问题：
1. **串行解析流程** - 函数分析、上下文提取、依赖分析完全串行执行
2. **重复AST解析** - 每个函数都重复解析整个文件的AST
3. **跨模块调用分析低效** - 对每个函数都遍历所有编译单元
4. **内存占用过高** - 同时保存所有函数的完整上下文
5. **LLM调用阻塞** - 所有解析完成后才开始LLM调用

### 主要耗时组件：
- `ClangAnalyzer.analyze_file()` - AST解析（最耗时）
- `CallAnalyzer.find_call_sites()` - 跨文件调用分析（次耗时）
- `FunctionAnalyzer._analyze_function_context()` - 依赖分析

## 🚀 流水线式优化方案

### 核心设计：生产者-消费者流水线

```
[文件解析] → [函数队列] → [上下文分析] → [LLM生成] → [结果输出]
    ↓           ↓           ↓            ↓           ↓
  Worker1     Worker2     Worker3     Worker4     Worker5
```

### 关键优化策略：

#### 1. **分阶段流水线处理**
- **阶段1**: 快速函数发现（ lightweight 解析）
- **阶段2**: 按需上下文提取（lazy loading）
- **阶段3**: 并行LLM生成（continuous processing）
- **阶段4**: 流式结果输出（immediate write）

#### 2. **智能缓存与共享**
- 文件级AST缓存（避免重复解析）
- 跨函数调用分析缓存
- 增量依赖分析

#### 3. **优先级队列调度**
- 简单函数优先（快速出结果）
- 复杂函数并行处理
- 失败任务自动重试

## 📋 具体重构实现计划

### Phase 1: 核心流水线架构 (3-4天)

#### 1.1 创建流式处理组件
```python
# src/core/pipeline/
├── __init__.py
├── function_streamer.py      # 函数发现器
├── context_processor.py      # 上下文处理器
├── llm_processor.py          # LLM处理器
├── result_collector.py       # 结果收集器
└── pipeline_orchestrator.py  # 流水线协调器
```

#### 1.2 实现函数快速发现
- 轻量级AST遍历，仅提取函数签名
- 立即将函数放入处理队列
- 延迟详细上下文分析

#### 1.3 设计队列系统
- 使用 `queue.Queue` 实现多级缓冲
- 支持优先级调度
- 异常处理和重试机制

### Phase 2: 智能上下文分析 (2-3天)

#### 2.1 优化AST解析
```python
class ASTCache:
    """文件级AST缓存，避免重复解析"""
    def get_or_parse(self, file_path: str, compile_args: List[str]):
        # 检查缓存
        # 按需解析
        # 返回共享AST
```

#### 2.2 增量依赖分析
- 共享跨函数调用分析结果
- 增量式宏定义收集
- 按需数据结构提取

### Phase 3: 并行LLM处理 (2天)

#### 3.1 实现连续LLM调用
- 函数解析完成立即发送LLM
- 支持动态并发数调整
- API限流和错误重试

#### 3.2 流式结果输出
- LLM响应完成立即写入文件
- 减少内存占用
- 实时进度反馈

### Phase 4: 配置与监控 (1-2天)

#### 4.1 性能配置
```yaml
pipeline:
  max_workers: 3
  queue_size: 100
  # batch_size removed - using streaming
  ast_cache_enabled: true
  continuous_llm: true

performance:
  function_priority: "simple_first"  # simple_first, complex_first, none
  memory_limit_mb: 1024
  timeout_per_function: 300
```

#### 4.2 实时监控
- 各阶段处理速度监控
- 内存使用跟踪
- 瓶颈识别和报警

## 🎯 预期性能提升

### 时间优化：
- **总体时间减少 60-70%**
- **首函数输出时间减少 80%**（从分钟级到秒级）
- **内存使用减少 40-50%**

### 关键改进：
1. **即时反馈** - 简单函数几秒内生成测试
2. **流畅体验** - 实时进度显示，不会长时间卡顿
3. **资源高效** - 避免重复解析，减少内存峰值
4. **容错性强** - 单个函数失败不影响整体流程

### 向后兼容：
- 保持现有API接口不变
- 配置开关控制新旧模式
- 渐进式迁移，降低风险

## 🚀 实施建议

1. **优先实现核心流水线** - 先保证基本流程跑通
2. **渐进式优化** - 从最简单的缓存开始，逐步添加高级特性
3. **保持旧接口** - 确保现有功能不受影响
4. **充分测试** - 每个阶段都要验证功能正确性

## 📊 技术细节

### 核心类设计预览

```python
class PipelineOrchestrator:
    """流水线协调器"""
    def __init__(self, config):
        self.function_streamer = FunctionStreamer(config)
        self.context_processor = ContextProcessor(config)
        self.llm_processor = LLMProcessor(config)
        self.result_collector = ResultCollector(config)

    async def run_pipeline(self, compilation_units):
        """运行流水线处理"""
        # 启动所有workers
        # 协调各阶段处理
        # 监控和错误处理
```

### 队列系统设计

```python
class ProcessingQueue:
    """多级处理队列"""
    def __init__(self, max_size=100):
        self.discovery_queue = Queue(max_size)    # 函数发现队列
        self.context_queue = Queue(max_size)      # 上下文处理队列
        self.llm_queue = Queue(max_size)          # LLM处理队列
        self.result_queue = Queue(max_size)       # 结果收集队列
```

## 📈 监控指标

- **吞吐量**: 每分钟处理的函数数量
- **延迟**: 从发现到生成的平均时间
- **成功率**: LLM调用成功率
- **资源使用**: CPU、内存占用情况

通过这个优化方案，ai-dt项目将能够提供更流畅的用户体验，特别是对于大型C/C++项目的测试生成场景。